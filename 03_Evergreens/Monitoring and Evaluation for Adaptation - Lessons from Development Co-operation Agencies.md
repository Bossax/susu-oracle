---
type: literature
status:
  - current
tags:
  - Monitoring_and_Evaluation
  - result-based
  - Baseline_framework
source:
authors: []
year:
relevance_to: "[[]]"
key_findings: []
---
 **OECD** working paper, "Monitoring and Evaluation for Adaptation: Lessons from Development Co-operation Agencies,
# Result based management
Results-Based Management (RBM) is defined as a **management strategy that focuses on the performance and achievement of outputs, outcomes, and impacts**. It is commonly employed by development co-operation agencies to design and manage their projects and programmes.

## Components:
1. **Implementation measurements**: These are used to ensure that project or programme inputs and activities comply with the designed budget and work plan.
2. **Result measurements**: This component involves examining the achievement of project objectives in terms of immediate outputs, intermediate outcomes, and long-term impacts.

For the result measurement component, the #LogicalFrameworkApproach(LFA) and its accompanying logframe are commonly used. The logframe, often a four-by-four matrix, summarizes core project components by listing a vertical hierarchy of objectives (activities, outputs, outcomes, goal) and presenting how each objective will be assessed, including means of assessment and assumptions.

Within the RBM framework, the levels of achievement are defined as:
- **Output**: The products, capital goods, and services that result directly from a development intervention, which may also include changes relevant to achieving outcomes.
- **Outcome**: The likely or achieved **short-term and medium-term** effects stemming from an intervention’s outputs.
- **Impact**: The **long-term** effects, both positive and negative, primary and secondary, produced by a development intervention, whether directly or indirectly, intended or unintended. 

**Limitations of Output Measurement Alone:**
- **Do not guarantee implementation or sustainability**: The development of a policy framework (output) does not guarantee its implementation or long-term sustainability. It "needs to be complemented with quantitative indicators that for example measure the number of projects that have been developed in response to the policy or the number of households benefitting," as well as "qualitative indicators... to assess the change brought about by the policy".
- **Risk of oversimplification**: Simple binary indicators, like "mechanisms for risk management integrated into the poverty reduction strategy => yes/no," may indicate progress on paper but not actual implementation or impact.
- **Do not explain 'how' change occurred**: Indicators provide an overview of change but "do not explain how that change came about".
- **Limited insight into adaptive capacity**: While output indicators can show training sessions conducted or materials published, they "do not capture the impact of these activities on adaptive capacity" or the "internalization of the training" by beneficiaries. Direct consultation with beneficiaries is often needed to understand changes in livelihood choices or the reasons for behavioral shifts.
## a. Indicator 
A core aspect of RBM, employed by most development agencies, is the **selection of concise and measurable indicators**. While outputs and outcomes outline what an activity aims to achieve, indicators specify _how_ results will be measured. In the context of adaptation initiatives, where a long-term perspective is crucial, it is particularly important to clearly differentiate between outcomes, outputs, and activities. This differentiation helps to clarify the relative contribution of each activity towards the long-term objective.

For adaptation projects, RBM frameworks typically combine **qualitative, quantitative, and binary indicators**. No single category of indicator is sufficient on its own. For instance, the creation of a policy framework needs to be complemented with quantitative indicators that measure its implementation and sustainability, such as the number of projects developed in response to the policy or the number of households benefiting. Qualitative indicators are also vital for assessing the change brought about by a policy.

### i) Selection of Indicators
The selection of indicators is a **core component** of RBM's monitoring and evaluation (M&E) framework.
#### 1) Alignment and Measurability
Indicators are chosen to demonstrate **how results will be measured**. While outputs and outcomes define what an activity aims to achieve, indicators specify the means of measurement. There is a preference for **concise and measurable indicators**. However, there is a practical challenge: the requirement for "objectively verifiable indicators" can sometimes lead to the use of indicators that are **easily measurable within the project's timeframe** rather than those most closely aligned with the intended outcomes.

**Activity Type**: The **type of activity will largely determine the choice of indicators**.

#### 2) Combination of Indicator Types
For robust M&E, particularly in adaptation initiatives, frameworks should **combine qualitative, quantitative, and binary indicators**, as no single category is sufficient on its own. For instance, a policy framework's development (binary) needs to be complemented by quantitative indicators measuring its implementation (e.g., number of projects developed in response) and qualitative indicators assessing the change it brought about.

#### 3) Clarity and Comparability
-Indicators need to be **carefully defined** to ensure they can monitor progress and evaluate results. Vague terms like "adaptive capacity," "ability," or "robustness" require **clear guidelines or scoring systems** for unbiased evaluation. It is also crucial that the **means of collecting indicators remain constant over time** to ensure comparability, especially for long-term projects.


### ii) Baselines #Baseline_framework 
#### 1) Definition and Purpose
Baseline data is a fundamental component of any evaluation as

it provides a reference point against which results can be measured.

#### 2) Challenges in Setting Baselines for Adaptation 
**Changing Baselines**:
A key challenge in setting baselines for climate change intervention is that the **baseline itself will inherently change over time**. Historical baselines can become misleading as climate change progresses.

**Distinguishing Climate Trends from Natural Variability**: The weather experienced at any given time results from a combination of climate trends and natural variability, complicating baseline establishment.

**Uncertainty of Extreme Events**: Some adaptation activities can only be evaluated after extreme climate events, the timing of which is uncertain. For example, a flood management project's success cannot be assumed solely because an area hasn't flooded.

#### 3) Incorporating Future Climate Projections
Projects should **use climate projections to account for how the climate will vary over the project's life**. This can be complemented by analyzing past trends.
**Climate models are a useful tool** for establishing projected baselines and targets, offering information on climate scenarios, present and future vulnerabilities, and adaptation costs and uncertainties.
While targets should consider present and future benefits, **relevant uncertainties must be taken into account**, especially when projections differ significantly. In such cases, "no-regret" adaptation measures might be necessary.

>[!warning] Current Practice in the Sample
Despite the importance of climate models for baselines, the **projects and programmes analyzed in this study did not explicitly draw on climate models to establish baselines and targets**. However, the development of climate scenarios and integration of research results into decision-making are integral components of many projects.
Only one JICA project ("Countermeasures for sediment in Wonogiri Multipurpose Dam reservoir") uses **projections for its baseline and target**. This project's main indicator measures sedimentation volume, with baseline and target values based on projected figures for 2014. However, the two-year post-completion timeline for this assessment is noted as a limitation, as it's too early for climate impacts to materialize.
    

### When and How Indicators are Obtained

Indicators are obtained through a systematic collection of data for monitoring and evaluation purposes.

- **Ongoing Monitoring**: "Monitoring" is defined as the **systematic collection of data on pre-defined project or programme indicators** to check if an initiative is on track. This implies continuous or regular data collection throughout the project's life.
- **Annual Reviews**: Agencies like CIDA produce **annual performance reports** assessing results achieved to date. DFID annually reviews multi-year activities, where project managers record progress against objectives in formats like Excel sheets.
- **Project Phases**: Data collection aligns with the project's hierarchy of objectives. For instance, the **Logical Framework Approach (LFA)** includes columns for "Means of verification" for activities, outputs, outcomes, and goals, indicating how and when each objective's achievement will be assessed.
- **Evaluations**: Evaluators draw on objectives and means of evaluation outlined in logframes, complemented by information from **project documents and interim reports** produced during monitoring activities. Evaluations measure change over time and assess project design strengths and weaknesses.
- **Adjusted Timing for Adaptation**: For adaptation initiatives, the **timing of monitoring and evaluation activities needs to be adjusted to the longer time horizon** of potential climate change impacts. This may necessitate **additional evaluations planned after the project end date** to verify longer-term impacts.
- **Data Collection Methods**: Methods for obtaining indicator data include:
    - **Direct consultation with beneficiaries** through means like **male and female focus group discussions**.
    - **Beneficiary surveys**.
    - Review of **training records, programme surveys, field monitoring of practices**, and M&E system data.
    - Examination of **disaster preparedness and response plans**.
    - Reviews of **grantee agreements**.
    - Analysis of **seminar materials, facilitator reports, and structured participant evaluations**.
    - Consultation of **publicly available information materials** and **provincial training records**.
    - Accessing **knowledge platforms and websites**.
    - Review of **synthesized reports** and records of exchanges/meetings.

The process requires clearly defined data sources and roles/responsibilities for data collection.
## b. Implementation measurements
Implementation measurements ensure that **project or programme inputs and activities are in compliance with the designed budget and work plan**. This contrasts with "result measurements," which focus on achieving immediate outputs, intermediate outcomes, and long-term impacts.

- **Check compliance to project standard and plan:** the purpose of implementation measurements is to verify compliance, implying a focus on tracking inputs and activities against plans. The Logical Framework Approach (LFA) and its accompanying logframe are "commonly used" within RBM. The logframe, a four-by-four matrix, outlines "Activities" and "Inputs" as the lowest levels of objectives. For "Activities," it includes "Measures (direct or indirect) to show if project outputs are being delivered" and "Sources of information and methods used to show that activities have been completed". For "Inputs," it lists "Resources – type and level of non-financial resources needed for the project Finance – overall budget Time – planned start and end date". This structure suggests that monitoring involves checking these elements against their planned delivery and resource allocation.

## c. Output measurement 

**output** = "the products, capital goods and services which result from a development intervention; may also include changes resulting from the intervention that are relevant to the achievement of outcomes".

Historically, development agencies primarily focused on measuring **outputs** because they had more direct control over them compared to outcomes, which depend on a range of external factors. For instance, constructing a school is an output directly resulting from donor investment, whereas the number of children attending that school is an outcome influenced by various factors beyond the project's direct control.

However, the sources highlight a shift towards emphasizing outcomes, as **a narrow focus solely on outputs risks overstating results** if the desired long-term change does not materialize. For example, if a newly built school remains empty, the impact is limited, or if training is not applied, there is no sustainable change.

**Measuring Outputs:**
> [!note] Outputs define "what the activity hopes to achieve".

- Output indicators are often **quantitative** (e.g., number of bridges constructed) or **binary** (e.g., yes/no for policy development).
    - Examples of common output indicators identified in the sources include:
        - "No. of individuals in targeted communities developing resilient strategies".
        - "No. of communities sensitised to DRR and climate change; with disaster preparedness and response plans".
        - "No. of Village Savings and Loans Associations in place and operating".
        - "Optimal number of agro-met stations established to service project villages".
        - "Protocols and tools for water-budgeting developed".
        - "Local disaster management plans exist and put in place".
        - "No. of advisories on water use, crop planning and management; pest management, etc. issued".
        - "No. and type of [disaster risk reduction] instruments e.g. insurance instruments promoted".
        - "Number of educational materials produced".
        - "Number of training sessions conducted".
        - "Percentage of households with new livelihood activities resulting from the project".
- The level of detail in output indicators can vary by agency and project scale. Some agencies have "detailed indicators corresponding to every component of an intervention", while others focus on "a few indicators relevant to one or two key aspects".
- Output indicators, such as the number of climate strategies, should be referenced against a baseline, like the "total number of villages vulnerable to climate change at the start of the programme".

**Limitations of Output Measurement Alone:**
• **Do not guarantee implementation or sustainability**: The development of a policy framework (output) does not guarantee its implementation or long-term sustainability. It "needs to be complemented with quantitative indicators that for example measure the number of projects that have been developed in response to the policy or the number of households benefitting," as well as "qualitative indicators... to assess the change brought about by the policy".

• **Risk of oversimplification**: Simple #binary_indicator, like "mechanisms for risk management integrated into the poverty reduction strategy... yes/no " may indicate progress on paper but not actual implementation or impact.

• **Do not explain 'how' change occurred**: Indicators provide an overview of change but "do not explain how that change came about".

• **Limited insight into** #adaptive_capacity: While output indicators can show training sessions conducted or materials published, they "do not capture the impact of these activities on adaptive capacity" or the "internalisation of the training" by beneficiaries. Direct consultation with beneficiaries is often needed to understand changes in livelihood choices or the reasons for behavioral shifts.





# #LogicalFrameworkApproach 
### Objective of the Logical Framework Approach (LFA)

The LFA serves as a useful analytical and organizational framework for summarizing core project components. When carefully managed, it provides a structured way to outline what an activity hopes to achieve and how results will be measured.

> [!important] It aids the project design and implementation

### How it is Used

1. **As a planning and M&E tool**: The LFA is a standard tool for **planning projects**, and in some cases, also for providing the **monitoring and evaluation framework**. It helps evaluators draw on objectives and means of evaluation, complemented by information from project documents and interim reports.
2. **For comparison**: For donors or implementing partners, the LFA provides a **common basis for comparison across interventions**.
3. **To meet donor requirements**: A logframe might be produced simply to meet **donor requirements for funding proposals**, sometimes without a participatory process.
4. **As a way of thinking**: Stakeholders may use the LFA to agree on program components, objectives, indicators, and assumptions, even without producing a physical logframe, emphasizing its utility as a **tool for working through a hierarchy of objectives, risks, and assumptions**.
5. **For differentiating results**: In adaptation initiatives, the LFA helps to **clearly differentiate between outcomes, outputs, and activities**, which is particularly important due to the long-term perspective of these initiatives. This differentiation clarifies each activity's contribution to the long-term objective.

### How it Works

The findings from an LFA are usually summarized in a **four-by-four matrix, called a logframe**.

- **Hierarchy of Objectives**: The **rows** of the logframe list the **vertical hierarchy of objectives**:
    1. **Activities** deliver outputs.
    2. **Outputs** contribute to outcomes.
    3. **Outcomes** help bring about the overall **goal**.
    
- **Assessment and Verification**: The **columns** present **how each objective will be assessed** and the **means of assessment**. This includes:
    1. **Objectively verifiable indicators**: Measures (direct or indirect) to show progress or delivery at each level (goal, outcomes, outputs, activities).
    2. **Means of verification**: Sources of information and methods used to show fulfillment or completion.
    3. **Assumptions**: Important events, conditions, or decisions beyond the project's control necessary for maintaining progress or achieving objectives/outputs.
- **Inputs**: The logframe also lists **inputs**, including:
    1. **Resources**: Type and level of non-financial resources needed.
    2. **Finance**: Overall budget.
    3. **Time**: Planned start and end date.
    
- **Supporting Tools**: To be effective, the LFA needs to employ tools such as institutional capacity assessments, economic and financial analysis, and environmental institutional assessments.
- **DFID's Revised Format**: DFID's revised logframe format (post-2009) includes a brief description, indicators, **baseline values, milestones, targets**, assumptions, data sources, roles and responsibilities, and input values. It also identifies the percentage weight allocated to each output and outlines **possible risk factors** for achieving objectives instead of assumptions. This separation of baselines, milestones, and targets is useful for monitoring, especially as baseline climatic risks can evolve.

### Case of this Approach

The LFA and logframes are **the most common M&E approaches used for adaptation** by development co-operation agencies.

- **Prevalence**: Most agencies examined in the study use varying formats of logframes.
- **Challenges**: Despite its common use, there are **challenges in applying logframes and the LFA** to development interventions. These include:
    - **Accounting for unexpected outcomes**.
    - **Measuring attribution**: Longer-term impacts are unlikely to result from the project alone, making it hard to attribute outcomes to a specific intervention (the "attribution gap").
    - **Indicator selection bias**: The requirement for "objectively verifiable indicators" can lead to using indicators that are **easily measurable within the project's timeframe** rather than those most closely aligned with intended outcomes.
- **Differentiation and Flexibility**: Clear differentiation between outputs, outcomes, and activities within the logframe helps clarify the relative contribution of each component. While some agencies use detailed logframes, others like JICA focus on a few key indicators, allowing for flexibility to revise project components based on the ground situation. A certain level of flexibility is needed to ensure all relevant issues are considered.
- **Importance of Baselines and Targets**: Careful definition of baselines, intermediate milestones, and targets is crucial for objective evaluation of progress. For adaptation, baselines must include the effects of future climate change, particularly for long-term projects like infrastructure investments.